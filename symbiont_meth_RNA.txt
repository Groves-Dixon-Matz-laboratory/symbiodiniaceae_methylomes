#invert_meth_ge_RNAseq_walkthough.txt
#Steps for processing RNAseq samples
#see invert_meth_ge_setup_walkthough.txt for preparing data for steps below
#Groves Dixon
#2-11-20


##################################
#### GET SYMBIONT PROPORTIONS ####
##################################

#subset 100 thousand reads for each trim file
mkdir zooxProp
for file in *.trim
do head -n 400000 $file > zooxProp/${file}
done

#map as below to the cladesABC reference
source /work/02260/grovesd/lonestar/myReferences/cladeABCD_genomes_longerThan1Kb.sh    #cladesABCD

#PAIRED END
module load bowtie
>mappe
for file in *_2.trim
do echo "\
bowtie2 -x $GENOME_PATH -1 ${file/_2.trim}_1.trim -2 ${file} --local -S ${file/_2.trim}.sam" >> mappe
done

#SINGLE END
>mapse
for file in *.trim
do echo "\
bowtie2 -x $GENOME_PATH -U ${file} --local -S ${file/.trim/}.sam">> mapse
done


#count the alignments from the sam files
MINQ=40
for file in *.sam
do echo "samtools view -F 256 -q $MINQ $file | singleFile_symbiont_typer.awk > ${file/.sam/}_symbiont_counts.txt &"
done

#assemble
echo -e "all\tnonSym\tcladeA\tcladeB\tcladeC\tcladeD" > sym_clade_counts.tsv
for file in *symbiont_counts.txt
do awk -v FILE=$file 'BEGIN {OFS="\t"} {print $0,FILE}' $file >> sym_clade_counts.tsv
done

#################################
############ MAPPING ############
#################################

#choose the genome you're working with
source /work/02260/grovesd/lonestar/myReferences/aiptasia_cladeA_reference.sh          #Aiptasia-cladeA
source /work/02260/grovesd/lonestar/myReferences/aiptasia_cladeA_reference.sh          #Aiptasia-cladeB #This actually seems to be the dominant one
source /work/02260/grovesd/lonestar/myReferences/Amil.v2.00_chrs_cladeC_genomes.sh     #Amillepora-cladeC
source /work/02260/grovesd/lonestar/myReferences/stylophora_cladeA_genomes.sh          #Stylophora-cladeA  



#check genome load
echo $GENOME_PATH
less $GENOME_PATH

#PAIRED END
module load bowtie
>mappe
for file in *_2.trim
do echo "\
bowtie2 -x $GENOME_PATH -1 ${file/_2.trim}_1.trim -2 ${file} --local -p 6 -S ${file/_2.trim}.sam" >> mappe
done

launcher_creator.py -n mapAipRNA -j mappe -q normal -N 3 -w 4 -a $allo -t 24:00:00 -e $email
#with -p 6 for bowtie, wayness of 6 seems good (36 total out of 48, leaving some extra memory), assuming 12 pairs of .trim files to map, 2 nodes lets them all map at once
#This sometimes finishes as fast as a couple hours, but can take much longer for large .trim files, could be fancy to set the amount of time requested based on the read counts

#SINGLE END
>mapse
for file in *.trim
do echo "\
bowtie2 -x $GENOME_PATH -U ${file} --local -p 3 -S ${file/.trim/}.sam">> mapse
done

launcher_creator.py -n mapAmilClades -j mapse -q normal -N 1 -w 12 -a $allo -e $email -t 12:00:00




###########################################
##### PREPARE ALIGNMENTS FOR COUNTING #####
###########################################

#SORT BY COORDIANTE, REMOVE DUPLICATES, THEN CONVERT BACK TO SAM FOR COUNTING

###!!! NOTE! REMOVING PCR DUPLICATES MAY DO MORE HARM THAN GOOD:
#(https://dnatech.genomecenter.ucdavis.edu/faqs/should-i-remove-pcr-duplicates-from-my-rna-seq-data/)
#https://www.biostars.org/p/55648/   and these excellent papers
#Parekh et al 2016:  The impact of amplification on differential expression analyses by RNA-seq.  and
#Fu et al. 2018:  Elimination of PCR duplicates in RNA-seq and small RNA-seq using unique molecular identifiers.
# THE STEPS HERE ARE LEFT FOR REFERENCE, BUT MAYBE DON'T TO DO THIS?
# INSTEAD, JUST SORT THEM AND GO STRAIGHT TO FEATURECOUNTS


#These are fast, but removing duplicates takes a lot of memory

module load samtools
>removeDups
for file in *.sam
do runID=${file/.sam/}
 echo "samtools sort -O bam -o ${runID}_sorted.bam $file &&\
 java -Xms4g -jar /work/02260/grovesd/lonestar/picard/picard-tools-1.119/MarkDuplicates.jar\
 INPUT=${runID}_sorted.bam\
 OUTPUT=${runID}_dupsRemoved.bam\
 METRICS_FILE=${runID}_dupMetrics.txt\
 REMOVE_DUPLICATES=true &&\
 samtools index ${runID}_dupsRemoved.bam" >> removeDups
 done
 
launcher_creator.py -n rDupSAmil -j removeDups -t 2:00:00 -q development -a $allo -e $email -N 2 -w 2
##run only two per node, assuming 12 sam files, using six nodes allows for all to run simultaneously, expect run to last ~2-4 hours


######################################
############# GET COUNTS #############
######################################

#choose the genome you're working with
source /work/02260/grovesd/lonestar/myReferences/aiptaisiaReference.sh    #aiptaisia
source /work/02260/grovesd/lonestar/myReferences/amilleporaReference.sh   #amillepora
source /work/02260/grovesd/lonestar/myReferences/bumbleBeeReference.sh    #bumblebee
source /work/02260/grovesd/lonestar/myReferences/maternalBeeReference.sh  #maternal bee
source /work/02260/grovesd/lonestar/myReferences/robustAntReference.sh    #robust ant
source /work/02260/grovesd/lonestar/myReferences/silkwormReference.sh     #silkworm
source /work/02260/grovesd/lonestar/myReferences/stylophoraReference.sh   #S. pistillata
source /work/02260/grovesd/lonestar/myReferences/honeybeeReference.sh     #honeybee
source /work/02260/grovesd/lonestar/myReferences/termiteReference.sh      #termite


#run featurecounts
echo "featureCounts -a $GFF_PATH -p -t gene -g $GENE_ID -o feature_counts_out.tsv -T 36 --primary *_dupsRemoved.bam" > runFeatureCounts





#fix names eg:
sed -i.bak 's/_dupsRemoved.bam//g' feature_counts_out.tsv




#######################################
####### PIPELINE COUNTS RESULTS #######
#######################################


wc -l *.fastq |\
 awk '{split($2, a, ".fastq")
 print a[1]"\t"$1/4"\trawCounts"}' |\
 grep -v total > raw_read_counts.tsv &




#GET POST TRIMMING READ COUNT
wc -l *.trim |\
 awk '{split($2, a, ".trim")
 print a[1]"\t"$1/4"\ttrimmedCounts"}' |\
 grep -v total > trimmed_read_counts.tsv &


#get alignment counts before removal
>getInitialAlignment
for file in *sorted.bam
do echo "samtools flagstat $file > ${file/_sorted.bam/}_prededup_flagstats.txt &" >> getInitialAlignment
done

#get post removal alignment counts
>getDupRemAlignment
for file in *dupsRemoved.bam
do echo "samtools flagstat $file > ${file/.bam/}_post_dedup_flagstats.txt &" >> getDupRemAlignment
done



#format properly paired reads
>prededup_properly_paired_count.tsv
for file in *prededup_flagstats.txt
do pp=$(grep "properly paired" $file); echo -e "$file\t$pp" |\
 awk '{split($1, a, "_prededup_flagstats.txt")
 split($7, b, "(")
 print a[1]"\t"$2"\tpredupPropPaired"}' >> prededup_properly_paired_count.tsv
 done

#format total reads
>prededup_mapped_count.tsv
for file in *prededup_flagstats.txt
do pp=$(grep "mapped" $file | head -n 1)
 echo -e "$file\t$pp" |\
 awk '{split($1, a, "_prededup_flagstats.txt")
 print a[1]"\t"$2"\tpredupMapped"}' >> prededup_mapped_count.tsv
 done


#removal metrics
>dupRemovalMetrics.tsv
for file in *dupMetrics.txt
do pct=$(sed '8q;d' $file | cut -f 8)
echo -e "$file\t$pct" |\
 awk '{split($1, a, "_dupMetrics.txt")
 print a[1]"\t"$2"\tdupRemProp"}' >> dupRemovalMetrics.tsv
done


#format properly paired reads
>dedup_properly_paired_count.tsv
for file in *_post_dedup_flagstats.txt
do pp=$(grep "properly paired" $file)
 echo -e "$file\t$pp" |\
 awk '{split($1, a, "_dupsRemoved_post_dedup_flagstats.txt")
 print a[1]"\t"$2"\tdedupPropPair"}' >> dedup_properly_paired_count.tsv
done

#format total reads
>dedup_mapped_count.tsv
for file in *_post_dedup_flagstats.txt
do pp=$(grep "mapped" $file | head -n 1)
 echo -e "$file\t$pp" |\
 awk '{split($1, a, "_dupsRemoved_post_dedup_flagstats.txt")
 print a[1]"\t"$2"\tdedupMapped"}' >> dedup_mapped_count.tsv
 done


#COUNTED ON GENES
total_gene_counts_featureCounts.R feature_counts_out.tsv



#ASSEMBLE
cat raw_read_counts.tsv trimmed_read_counts.tsv prededup_properly_paired_count.tsv dedup_properly_paired_count.tsv gene_count_sumsFC.tsv > pipelineCounts.tsv


########################################
######## ORGANIZE SAMPLE TRAITS ########
########################################

#first wrangle sample data with wrangle_sample_data.R


######################################################
### INITIALIZE COUNTS/CONTROL FOR COVARIATES/DESEQ ###
######################################################
#note the old way I did this is still included below

#SETUP
#move all the *Coldata.csv files output from wrangle_sample_data.R to TACC

#grab raw counts from corral
cp /corral-repl/utexas/tagmap/dixon_backups/gene_expression_meta/dupRemoval/featureCounts/all_featureCounts_geneCounts_laneDupsRemd.tsv .


#check the coldata files:
ll *.csv | wc -l
ll *.csv | awk '{split($9, a, "_"); print a[1]}' | sort | uniq 
ll *.csv | awk '{split($9, a, "_"); print a[1]}' | sort | uniq | wc -l


#BUILD COMMANDS

#basic command looks like this:
#FOR BLEACHED:
initialize_raw_featureCounts.TACC_V2.R --counts $COUNTS --coldat BEWW_Coldata.csv --treeCut 5000 --mnCountCut 3 --callOutliers FALSE --o bleached &&\
 control_for_covariates.R --i bleached_wgcna_input.Rdata --o bleached &&\
 deseq_general.R --i bleached_deseqBaselineInput.Rdata --v bleached --t bleached --c control --o bleached


#build commands like this from each coldata file

ll *.csv | awk 'BEGIN {OFS=""};{split($9, a, "_"); print "\
initialize_raw_featureCounts.TACC_V2.R --counts all_featureCounts_geneCounts_laneDupsRemd.tsv --coldat ", $9, " --treeCut 5000 --mnCountCut 3 --callOutliers FALSE --o ", a[1], " &&\
 control_for_covariates.R --i ", a[1], "_wgcna_input.Rdata --o ", a[1], " &&\
 deseq_general.R --i ", a[1], "_deseqBaselineInput.Rdata --v stress --t stressed --c control --o ", a[1]}' > runDeseq


module load Rstats
launcher_creator.py -n runDeseq -j runDeseq -q normal -N 1 -w 5 -a $allo -e $email -t 04:00:00



#then run for individual projects for the all stress one
mkdir runIndividual
cd runIndidvidual
echo "deseq_general.R --i allStress_deseqBaselineInput.Rdata --v stress --t stressed --c control --o allStress --runInd TRUE" > runInd


#######################################
########## STRESS PREDICTION ##########
#######################################
#Run this is four batches:
	#1. BEWW spread by stress type, heat, cold, salinity, immune and pH using all other stress data as training set
	#2. BEWW spread by stress type, heat, cold, salinity, immune and pH each as training and testing set with each other
	#3. BEWW as its own, heatNoBEWW, coldNoBEWW, salinityNoBEWW, immune and ph each against all other stress
	#4. same as above each as training and test set

#First run on the stratified random sample for the full stess dataset. These were generated with wrangle_sample_data_specifics.R
stress_prediction.R --i stress_project_controlled.Rdata --train stratified_allStress_train.csv --test stratified_allStress_test.csv --o stratified_allStress_train.csv__stratified_allStress_test.csv


#Then run a new version with A-type stress and B-type stress included
stress_prediction.R --i stress_project_controlled.Rdata --train stratified_allStress_train.csv --test stratified_allStress_test.csv --o stratified_allStress_train.csv__stratified_allStress_test.csv




#SET UP PAIRS OF TRAINING AND TEST DATASETS FOR THE STRESS MINUS PAIRS
#these are equivalent to the *Coldata.csv files used for the specific DESeq runs above
#they are used to subset the normalized counts matrix made for all the stress experiments


nano mainPairs.txt
stressNOHEAT_Coldata.csv	heat_Coldata.csv
heat_Coldata.csv	stressNOHEAT_Coldata.csv
stressNOCOLD_Coldata.csv	cold_Coldata.csv
cold_Coldata.csv	stressNOCOLD_Coldata.csv
stressNOSALINITY_Coldata.csv	salinity_Coldata.csv
salinity_Coldata.csv	stressNOSALINITY_Coldata.csv
stressNOIMMUNE_Coldata.csv	immune_Coldata.csv
immune_Coldata.csv	stressNOIMMUNE_Coldata.csv
stressNOPH_Coldata.csv	ph_Coldata.csv
ph_Coldata.csv	stressNOPH_Coldata.csv



#build commands
mkdir mainPairs
awk ' BEGIN {OFS=""}{print "stress_prediction.R --i allStress_project_controlled.Rdata --train ", $1, " --test ", $2, " --o ", " mainPairs/",$1"__"$2"_predictions"}' mainPairs.txt > predMainPairs


#SET UP PAIRS OF TRAINING AND TEST DATASETS FOR THE STRESS MINUS PAIRS FOR THE BLEACHING SEPARATE GROUPS 
#(only need these for bleach, heat and salinity)
nano datasetPairsBleachSep.txt
stressNOBLEACH_Coldata.csv	bleached_Coldata.csv
bleached_Coldata.csv	stressNOBLEACH_Coldata.csv
stressNOHEAT_Coldata.csv	heatNOBLEACH_Coldata.csv
heatNOBLEACH_Coldata.csv	stressNOHEAT_Coldata.csv
stressNOSALINITY_Coldata.csv	salinityNOBLEACH_Coldata.csv
salinityNOBLEACH_Coldata.csv	stressNOSALINITY_Coldata.csv


#build commands
mkdir mainBleachSeperate
awk ' BEGIN {OFS=""}{print "stress_prediction.R --i allStress_project_controlled.Rdata --train ", $1, " --test ", $2, " --o ", " mainBleachSeperate/",$1"__"$2"_predictions"}' datasetPairsBleachSep.txt > predMainBleachSeperate


#SET UP PAIRS OF SPECIFIC STRESSES

nano specificStresses1.txt
heat_Coldata.csv
cold_Coldata.csv
salinity_Coldata.csv
immune_Coldata.csv
ph_Coldata.csv

pairup_list.py specificStresses1.txt permute > stressPairs1.tsv

#build commands
mkdir specResults
awk ' BEGIN {OFS=""}{print "stress_prediction.R --i allStress_project_controlled.Rdata --train ", $1, " --test ", $2, " --o ", "specResults/",$1"__"$2"_predictions"}' stressPairs1.tsv > specPredictions



#SET UP PAIRS OF SPECIFIC STRESSES NOBEWW
nano specificStressesBleachSep.txt
bleached_Coldata.csv
heatNOBLEACH_Coldata.csv
immune_Coldata.csv
ph_Coldata.csv
salinityNOBLEACH_Coldata.csv

pairup_list.py specificStressesBleachSep.txt permute > stressPairsBleachSep.tsv

mkdir specBleachSep
awk ' BEGIN {OFS=""}{print "stress_prediction.R --i allStress_project_controlled.Rdata --train ", $1, " --test ", $2, " --o ", "specBleachSep/",$1"__"$2"_predictions"}' stressPairsBleachSep.tsv > specPredBleachSep



cat predMainPairs predMainBleachSeperate specPredictions specPredBleachSep > allPreds



#######################################
################ WGCNA ################
#######################################


#Run this indev

#subset the variance stabilized counts for proportions of variance and expression value
#eg put -v 0.8 to get 80% of data with highest variances
subset_vsd_for_variance.R --v .75 --e 0.75 --i project_controlled.Rdata


#select the input
WGCNA_INPUT="project_controlled_varCut0.8_expCut0.8.Rdata"

#get soft threshold
echo "wgcna2_get_soft_threshold.R --input $WGCNA_INPUT --networkType signed" > getSoft


#run
echo "$runMyR ~/bin/wgcna3b_step-wise_network_construction.R \
 --softPower 12\
 --minSize 30\
 --mergeCutoff 0\
 --input $WGCNA_INPUT\
 --nCores 24\
 --networkType signed" > runWGCNA
 

#optionally re-run with merging


########################################
##### YEAST MAPPING AND ANNOTATION #####
########################################

#!! NOTE ENDED UP JUST USING THEIR WEBSITE !!
#left this just for reference

#here we get CDS sequences, annotate them with egg-mapper, then get read counts by modifying the transcriptome-based RNAseq pipeline here: https://github.com/z0on/tag-based_RNAseq/blob/master/tagSeq_processing_README.txt

#AQUIRE Lachancea kluyveri SEQUENCES:

#assembled a CDS file for Lachancea kluyveri from here: http://gryc.inra.fr/
#went to individual download section for Lachancea kluyveri here: http://gryc.inra.fr/index.php?page=download
#downloaded the CDS from each chromosome (SAKL0 A-H) and concatentated them into Lachancea_kluyveri_cds.fasta



#ANNOTATE THEM WITH GO AND KOG
#cloned eggnog-mapper to work
#following documentation here: https://github.com/eggnogdb/eggnog-mapper/wiki/eggNOG-mapper-v1

#DOWNLOAD AND DECOMPRESS DATABASE 
/work/02260/grovesd/lonestar/eggnog-mapper/download_eggnog_data.py --data_dir /scratch/02260/grovesd/eggnog_db


#HAD TO DOWNLOAD SOME MORE MANUALLY
wget http://eggnogdb.embl.de/download/emapperdb-4.5.1/OG_fasta.tar.gz

#If you want the euk database
wget -r http://eggnogdb.embl.de/download/eggnog_4.1/hmmdb_levels/euk_500/


#RUN ANNOTATIONS USING DIAMOND
split_fasta.py -i Lachancea_kluyveri_cds.fasta -n 300


NCPU=36
EGG_DB_DIR=$SCRATCH/eggnog_db
OUTDIR=./seed_orthologs
mkdir $OUTDIR

>runSeeds
for f in Lachancea_kluyveri_cds_split*.fasta; do
echo "python $WORK/eggnog-mapper/emapper.py --data_dir $EGG_DB_DIR --output_dir $OUTDIR --translate -m diamond --no_annot --no_file_comments --cpu $NCPU -i $f -o $f" >>runSeeds
done

#pick nodes equal to the number of chunks and to run each chunk with a full node and 32 cpus
launcher_creator.py -n runSeeds -j runSeeds -q normal -N 8 -w 4 -a $allo -e $email -t 12:00:00



#concatenate all the seed orthologs
cd $OUTDIR
cat *.fasta.emapper.seed_orthologs > allSeedsForAnnotaitons


#annotate them (this can be run in paralelle also?)
$WORK/eggnog-mapper/emapper.py --annotate_hits_table allSeedsForAnnotaitons --data_dir $EGG_DB_DIR --no_file_comments -o Lachancea_kluyveri_cds.emapper.diamond.annotations --cpu 36


#EXTRACT GO AND KOG


MY_ANNOTATIONS=Lachancea_kluyveri_cds.emapper.diamond.annotations.emapper.annotations


# GO:
awk -F "\t" 'BEGIN {OFS="\t" }{print $1,$7 }' $MY_ANNOTATIONS | grep GO | perl -pe 's/,/;/g' >${MY_ANNOTATIONS}_gene2go.tsv
# gene names:
awk -F "\t" 'BEGIN {OFS="\t" }{print $1,$22 }' ${MY_ANNOTATIONS} | grep -Ev "\tNA" >${MY_ANNOTATIONS}_gene2geneName.tsv

#  KOG classes (single-letter):
awk -F "\t" 'BEGIN {OFS="\t" }{print $1,$21 }' ${MY_ANNOTATIONS} > ${MY_ANNOTATIONS}_gene2kogClass1.tsv
# converting single-letter KOG classes to text understood by KOGMWU package (must have KOG_classes.txt file in the same dir):
awk 'BEGIN {FS=OFS="\t"} NR==FNR {a[$1] = $2;next} {print $1,a[$2]}' kog_classes.txt ${MY_ANNOTATIONS}_gene2kogClass1.tsv | awk '$2!=""' | grep -v "Function Unknown" > ${MY_ANNOTATIONS}_gene2kogClass.tsv


#TRIM AS ABOVE, BUT MAP SLIGHTLY DIFFERENTLY TO USE OLD TRANSCRIPTOME-BASED GENE COUNTING:

REFERENCE_CDS=$WORK/Lachancea_kluyveri_CDS/Lachancea_kluyveri_cds.fasta

>maps
for file in *.trim
do
echo "bowtie2 --local -x $REFERENCE_CDS -U $file -S ${file/.trim/}.sam --no-hd --no-sq --no-unal -k 5 -p 10" >>maps
done


#GET GENE COUNTS

#build dummy seq_to_gene.tab file
grep ">" $REFERENCE_CDS | sed 's/>//' | awk '{print $1"\t"$1}' > Lachancea_kluyveri_cds_seq2gene.tab


#get counts with Misha's perl script

samcount_launch_bt2.pl '\.sam' Lachancea_kluyveri_cds_seq2gene.tab > sc

# assembling all counts into a single table:
expression_compiler.pl *.sam.counts > yeast_cds_counts.tsv

#clean the junk off
sed -i 's/_1.sam.counts//g' yeast_cds_counts.tsv



########################################
##### REPEAT EGG MAPPER FOR A.MILL #####
########################################

#!! NOTE ENDED UP JUST USING THEIR WEBSITE !!
#left this just for reference

#concerned about disagreement between kogs and gos
#rerunning annotation pipeline as for yeast above from
#extracted protein sequences for A.mill.v2 reference

#setup
cd reAnnotateAmil
cp /work/02260/grovesd/lonestar/Amil_Zach_Fullers_v2.00/Amil.all.maker.proteins.fasta .


#RUN ANNOTATIONS USING DIAMOND
split_fasta.py -i Amil.all.maker.proteins.fasta -n 300


#------- RUN DIAMOND WAY -------#

#Set up variables for running
NCPU=12
EGG_DB_DIR=$SCRATCH/eggnog_db
DIAMOND_OUTDIR=./seed_orthologs_diamond
mkdir $DIAMOND_OUTDIR


#build commands to run diamond mode
>runDiamonSeeds
for f in Amilallmakerproteins_split*.fasta; do
echo "python $WORK/eggnog-mapper/emapper.py --data_dir $EGG_DB_DIR --output_dir $DIAMOND_OUTDIR -m diamond --no_annot --no_file_comments --cpu $NCPU -i $f -o $f" >>runDiamonSeeds
done

#pick nodes equal to the number of chunks and to run each chunk with a full node and 12 cpus
launcher_creator.py -n runDiamonSeeds -j runDiamonSeeds -q normal -N 5 -w 3 -a $allo -e $email -t 2:00:00

#concatenate all the seed orthologs
cd $DIAMOND_OUTDIR
cat *.fasta.emapper.seed_orthologs > allSeedsForAnnotaitons

#annotate them (this can be run in paralelle also?)
$WORK/eggnog-mapper/emapper.py --annotate_hits_table allSeedsForAnnotaitons --data_dir $EGG_DB_DIR --no_file_comments -o Amil.all.maker.proteins.diamond --cpu 36



#------- RUN HMMR WAY -------#

NCPU=40
EGG_DB_DIR=$SCRATCH/eggnog_db
HMM_DATABASE=/scratch/02260/grovesd/eggnog_db/hmmrDb/
HMM_OUTDIR=./seed_orthologs_hmm
mkdir $HMM_OUTDIR

#build commands to run HMMR mode
>runHmmrSeeds
for f in Amilallmakerproteins_split*.fasta; do
echo "python $WORK/eggnog-mapper/emapper.py --database euk --data_dir $HMM_DATABASE --output_dir $HMM_OUTDIR -m hmmer --no_annot --no_file_comments --cpu $NCPU -i $f -o $f" >>runHmmrSeeds
done

#pick nodes equal to the number of chunks and to run each chunk with a full node and 12 cpus
launcher_creator.py -n runHmmrSeeds -j runHmmrSeeds -q normal -N 5 -w 3 -a $allo -e $email -t 8:00:00



#concatenate all the seed orthologs
cd $HMM_OUTDIR
cat *.fasta.emapper.seed_orthologs > allSeedsForAnnotaitons


#annotate them (this can be run in paralelle also?)
$WORK/eggnog-mapper/emapper.py --annotate_hits_table allSeedsForAnnotaitons --data_dir $EGG_DB_DIR --no_file_comments -o Amil.all.maker.proteins.hmmr --cpu 36


#EXTRACT GO AND KOG

#NOTE!! ALL THESE HAVE THE -RA AT THE END, I DOUBLE-CHECKED AND THESE ARE EQUIVALENT TO THE GENE IDS, SO SIMPLY REMOVING THE -RA IS FINE TO GET TO GENE_IDS:



MY_ANNOTATIONS=Amil.all.maker.proteins.emapper.diamond.annotations.emapper.annotations


# GO:
awk -F "\t" 'BEGIN {OFS="\t" }{print $1,$7 }' $MY_ANNOTATIONS | grep GO | perl -pe 's/,/;/g' >${MY_ANNOTATIONS}_gene2go.tsv
# gene names:
awk -F "\t" 'BEGIN {OFS="\t" }{print $1,$22 }' ${MY_ANNOTATIONS} | grep -Ev "\tNA" >${MY_ANNOTATIONS}_gene2geneName.tsv

#  KOG classes (single-letter):
awk -F "\t" 'BEGIN {OFS="\t" }{print $1,$21 }' ${MY_ANNOTATIONS} > ${MY_ANNOTATIONS}_gene2kogClass1.tsv
# converting single-letter KOG classes to text understood by KOGMWU package (must have KOG_classes.txt file in the same dir):
awk 'BEGIN {FS=OFS="\t"} NR==FNR {a[$1] = $2;next} {print $1,a[$2]}' kog_classes.txt ${MY_ANNOTATIONS}_gene2kogClass1.tsv | awk '$2!=""' | grep -v "Function Unknown" > ${MY_ANNOTATIONS}_gene2kogClass.tsv



#NOTE ALL THESE HAVE THE -RA AT THE END, I DOUBLE-CHECKED AND THESE ARE EQUIVALENT TO THE GENE IDS, SO SIMPLY REMOVING THE -RA IS FINE TO GET TO GENE_IDS:



##################################
#EGG MAPPER CITATION INFORMATION:#
##################################
		CITATION:
		If you use this software, please cite:

		[1] Fast genome-wide functional annotation through orthology assignment by
			  eggNOG-mapper. Jaime Huerta-Cepas, Kristoffer Forslund, Luis Pedro Coelho,
			  Damian Szklarczyk, Lars Juhl Jensen, Christian von Mering and Peer Bork.
			  Mol Biol Evol (2017). doi: https://doi.org/10.1093/molbev/msx148

		[2] eggNOG 5.0: a hierarchical, functionally and phylogenetically annotated
			  orthology resource based on 5090 organisms and 2502 viruses. Jaime
			  Huerta-Cepas, Damian Szklarczyk, Davide Heller, Ana Hernandez-Plaza,
			  Sofia K Forslund, Helen Cook, Daniel R Mende, Ivica Letunic, Thomas
			  Rattei, Lars J Jensen, Christian von Mering and Peer Bork. Nucleic Acids
			  Research, Volume 47, Issue D1, 8 January 2019, Pages D309-D314,
			  https://doi.org/10.1093/nar/gky1085 
		[3] Accelerated Profile HMM Searches. PLoS Comput. Biol. 7:e1002195. Eddy SR.
			   2011.


		(e.g. Functional annotation was performed using emapper-1.0.3-40-g41a8498 [1]
		 based on eggNOG orthology data [2]. Sequence searches were performed
		 using [3].)



################################### APPENDIX FOR SYMBIONT GENE EXPRESSION ###################################
#this contains instructions for dealing with reads mapped to a concatenation of the Amil genome with symbiont C and D transcriptomes from Barshis

#------------ GET ZOOX PROPORTIONS ------------#
MINQ=40
>countSymb
for file in *_dupsRemoved.bam
do samtools view -F 256 -q $MINQ $file | singleFileZooxType_symCDtranscriptomes.awk > ${file/_dupsRemoved.bam/}_symbCounts.tsv" >>countSymb
done

#assemble the results
echo -e "file\tall\tnonSym\tcladeC\tcladeD" > all_zoox_type_counts.tsv
for file in *_symbCounts.tsv
do counts=$(cat $file)
echo -e "${file}\t${counts}" >> all_zoox_type_counts.tsv
done

#save all_zoox_type_counts.tsv in Acropora_gene_expression_meta/results_tables/



#------------ SPLIT UP FILES BY ZOOX TYPE ------------#

#FIRST GET BED FILES FOR THE TWO TRANSCRIPTOMES
REFERENCE=/work/02260/grovesd/stampede2/AmilSymCD_transcriptomes_seqs/Amil_SymCDtran.fasta
bed_from_fasta.py -fa $REFERENCE > Amil_SymCDtran.bed
grep "^c_sym_" Amil_SymCDtran.bed > symC.bed
grep "^d_sym_" Amil_SymCDtran.bed > symD.bed

#THEN SPLIT OUT THE SYMBIONT READS
>splitOutSymbs
for file in *_sorted.bam
do echo "\
samtools view -L symC.bed -o ${file/_dupsRemoved.bam/}_symC.bam $file
samtools view -L symD.bed -o ${file/_dupsRemoved.bam/}_symD.bam $file" >>splitOutSymbs
done


mkdir cladeC
mv *_symC.bam cladeC
mkdir cladeD
mv *_symD.bam cladeD


#------------ GET THE COUNTS ------------#

#RUN BEDTOOLS FOR EACH CLADE
cd cladeC
echo "bedtools multicov -bams *_symC.bam -bed symC.bed > symC_counts.tsv0" > getCounts

cd cladeD
echo "bedtools multicov -bams *_symD.bam -bed symD.bed > symD_counts.tsv0" > getCounts

#ASSEMBLE RESULTS

#for clade C
samples=$(ls *_symC.bam | sed 's/_sorted.bam_symC.bam//' | tr "\n" "\t")
echo -e "gene\tstart\tend\t${samples}" | sed "s/\t*$//g" > header.txt
cat header.txt symC_counts.tsv0 > symC_counts.tsv


#for clade D
samples=$(ls *_symD.bam | sed 's/_sorted.bam_symD.bam//' | tr "\n" "\t")
echo -e "gene\tstart\tend\t${samples}" | sed "s/\t*$//g" > header.txt
cat header.txt symD_counts.tsv0 > symD_counts.tsv
